{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mopet \ud83d\udef5 The mildly ominous parameter exploration toolkit Isn't it strange that, although parameter explorations are a crucial part of computational modeling, there are almost no Python tools available for making your life easier? mopet is here to help! You can run extensive grid searches in parallel (powered by ray ) and store extremely huge amounts of data into a HDF file (powered by pytables ) for later analysis - or whatever your excuse is for buying yet another hard disk. Installation \ud83d\udcbb The easiest way to get going is to install the pypi package using pip : pip install mopet Alternatively, you can also clone this repository and install all dependencies with git clone https://github.com/caglorithm/mopet.git cd mopet/ pip install -r requirements.txt pip install . Example usage \ud83d\udc1d Setting up an exploration is as easy as can be! # first we define an toy evaluation function def distance_from_circle ( params ): # let's simply calculate the distance of # the x-y parameters to the unit circle distance = abs (( params [ \"x\" ] ** 2 + params [ \"y\" ] ** 2 ) - # we package the result into a dictionary result = { \"result\" : distance } return result Let's set up the exploration by defining the parameters to explore and passing the evaluation function from above: import numpy as np import mopet explore_params = { \"x\" : np . linspace ( - 2 , 2 , 21 ), \"y\" : np . linspace ( - 2 , 2 , 21 )} ex = mopet . Exploration ( distance_from_circle , explore_params ) Running the exploration is in parallel and is handled by ray . You can also use a private cluster or cloud infrastructure, see here for more info. ex . run () >> 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 441 / 441 [ 426.57 it / s ] After your exploration has finished, you will find a file exploration.h5 in your current directory with all the runs, their parameters and their outputs, neatly organized. If you open this file (with HDFView for example), you'll see something like this: Loading exploration results You can load the exploration results using ex . load_results ( all = True ) Note that using all=True will load all results into memory (as opposed to just the parameters of each run). Please make sure that you have enough free memory for this since your simulation results could be huge. If you do not want this, you can load individual results using their run_id (which is an integer counting up one per run): ex . get_run ( run_id = 0 ) After using ex.load_results() , an overview of all runs and their parameters is given as a pandas DataFrame, available as ex.df . Using ex.load_results() with the default parameters will automatically aggregate all scalar results into this table, like distance in our example above, which is a float. Using some fancy pivoting, we can create a 2D matrix with the results as entries pivoted = ex . df . pivot_table ( values = 'result' , index = 'y' , columns = 'x' , aggfunc = 'first' ) Let's plot the results! import matplotlib.pyplot as plt plt . imshow ( pivoted , \\ extent = [ min ( ex . df . x ), max ( ex . df . x ), min ( ex . df . y ), max ( ex . df . y )], origin = 'lower' ) plt . colorbar ( label = 'Distance from unit circle' ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) More information \ud83d\udcd3 Inspired by \ud83e\udd14 mopet is inspired by pypet , a wonderful python parameter exploration toolkit. I have been using pypet for a very long time and I'm greatful for its existence! Unfortunately, the project is not maintained anymore and has run into several compatibility issues, which was the primary reason why I built mopet . Built With \ud83d\udc9e mopet is built on other amazing open source projects: ray - A fast and simple framework for building and running distributed applications. pytables - A Python package to manage extremely large amounts of data. tqdm - A Fast, Extensible Progress Bar for Python and CLI pandas - Flexible and powerful data analysis / manipulation library for Python numpy - The fundamental package for scientific computing with Python","title":"Home"},{"location":"#mopet","text":"The mildly ominous parameter exploration toolkit Isn't it strange that, although parameter explorations are a crucial part of computational modeling, there are almost no Python tools available for making your life easier? mopet is here to help! You can run extensive grid searches in parallel (powered by ray ) and store extremely huge amounts of data into a HDF file (powered by pytables ) for later analysis - or whatever your excuse is for buying yet another hard disk.","title":"mopet \ud83d\udef5"},{"location":"#installation","text":"The easiest way to get going is to install the pypi package using pip : pip install mopet Alternatively, you can also clone this repository and install all dependencies with git clone https://github.com/caglorithm/mopet.git cd mopet/ pip install -r requirements.txt pip install .","title":"Installation \ud83d\udcbb"},{"location":"#example-usage","text":"Setting up an exploration is as easy as can be! # first we define an toy evaluation function def distance_from_circle ( params ): # let's simply calculate the distance of # the x-y parameters to the unit circle distance = abs (( params [ \"x\" ] ** 2 + params [ \"y\" ] ** 2 ) - # we package the result into a dictionary result = { \"result\" : distance } return result Let's set up the exploration by defining the parameters to explore and passing the evaluation function from above: import numpy as np import mopet explore_params = { \"x\" : np . linspace ( - 2 , 2 , 21 ), \"y\" : np . linspace ( - 2 , 2 , 21 )} ex = mopet . Exploration ( distance_from_circle , explore_params ) Running the exploration is in parallel and is handled by ray . You can also use a private cluster or cloud infrastructure, see here for more info. ex . run () >> 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 441 / 441 [ 426.57 it / s ] After your exploration has finished, you will find a file exploration.h5 in your current directory with all the runs, their parameters and their outputs, neatly organized. If you open this file (with HDFView for example), you'll see something like this:","title":"Example usage \ud83d\udc1d"},{"location":"#loading-exploration-results","text":"You can load the exploration results using ex . load_results ( all = True ) Note that using all=True will load all results into memory (as opposed to just the parameters of each run). Please make sure that you have enough free memory for this since your simulation results could be huge. If you do not want this, you can load individual results using their run_id (which is an integer counting up one per run): ex . get_run ( run_id = 0 ) After using ex.load_results() , an overview of all runs and their parameters is given as a pandas DataFrame, available as ex.df . Using ex.load_results() with the default parameters will automatically aggregate all scalar results into this table, like distance in our example above, which is a float. Using some fancy pivoting, we can create a 2D matrix with the results as entries pivoted = ex . df . pivot_table ( values = 'result' , index = 'y' , columns = 'x' , aggfunc = 'first' ) Let's plot the results! import matplotlib.pyplot as plt plt . imshow ( pivoted , \\ extent = [ min ( ex . df . x ), max ( ex . df . x ), min ( ex . df . y ), max ( ex . df . y )], origin = 'lower' ) plt . colorbar ( label = 'Distance from unit circle' ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" )","title":"Loading exploration results"},{"location":"#more-information","text":"","title":"More information \ud83d\udcd3"},{"location":"#inspired-by","text":"mopet is inspired by pypet , a wonderful python parameter exploration toolkit. I have been using pypet for a very long time and I'm greatful for its existence! Unfortunately, the project is not maintained anymore and has run into several compatibility issues, which was the primary reason why I built mopet .","title":"Inspired by \ud83e\udd14"},{"location":"#built-with","text":"mopet is built on other amazing open source projects: ray - A fast and simple framework for building and running distributed applications. pytables - A Python package to manage extremely large amounts of data. tqdm - A Fast, Extensible Progress Bar for Python and CLI pandas - Flexible and powerful data analysis / manipulation library for Python numpy - The fundamental package for scientific computing with Python","title":"Built With \ud83d\udc9e"},{"location":"mopet/","text":"Mopet Exploration df property readonly Returns a dataframe with exploration results. Creates it from new if it doesn't exist yet. Returns: Type Description pandas.DataFrame Dataframe with exploration results __init__ ( self , function , explore_params , default_params = None , exploration_name = None , hdf_filename = None , num_cpus = None , num_gpus = None ) special Defines a parameter exploration of a given function . Parameters: Name Type Description Default function function Function to evaluate at each run required explore_params dict Exploration parameters (individual) for each run required default_params dict Default (shared) parameters to load for each run, optional, defaults to None None exploration_name str, optional Name of the run, will create a name if left empty, defaults to None None hdf_filename str, optional Filename of the hdf storage file, defaults to None None num_cpus int Number of desired CPU cores passed to ray, defaults to None None num_gpus int Number of desired GPUs passed to ray, defaults to None None Returns: Type Description Exploration instance Source code in mopet/mopet.py def __init__ ( self , function , explore_params , default_params = None , exploration_name = None , hdf_filename = None , num_cpus : int = None , num_gpus : int = None , ): \"\"\"Defines a parameter exploration of a given `function`. :param function: Function to evaluate at each run :type function: function :param explore_params: Exploration parameters (individual) for each run :type explore_params: dict :param default_params: Default (shared) parameters to load for each run, optional, defaults to None :type default_params: dict :param exploration_name: Name of the run, will create a name if left empty, defaults to None :type exploration_name: str, optional :param hdf_filename: Filename of the hdf storage file, defaults to None :type hdf_filename: str, optional :param num_cpus: Number of desired CPU cores passed to ray, defaults to None :type num_cpus: int, optional :param num_gpus: Number of desired GPUs passed to ray, defaults to None :type num_gpus: int, optional :return: Exploration instance \"\"\" self . function = function self . results = {} self . results_params = [] if default_params is not None : self . default_params = copy . deepcopy ( default_params ) self . full_params = True else : self . default_params = None self . full_params = False self . explore_params = copy . deepcopy ( explore_params ) if exploration_name is None : exploration_name = \"exploration\" + datetime . datetime . now () . strftime ( \"_%Y_%m_ %d _%HH_%MM_%SS\" ) self . exploration_name = exploration_name if hdf_filename is None : hdf_filename = \"exploration.h5\" self . hdf_filename = hdf_filename self . dfResults = None # status self . _hdf_open_for_reading = False # List of all parameter combinations generated when exploration starts self . explore_params_list = None # Dict with runId as keys and explored parameter dict as value. # Will be filled when exploration starts. self . run_params_dict = {} # Dict with runId as keys and explored parameter dict as value. # Will be filled when calling `load_results`. self . params = {} # Ray configuration self . num_gpus = num_gpus self . num_cpus = num_cpus close_hdf ( self ) Close a previously opened HDF file. Source code in mopet/mopet.py def close_hdf ( self ): \"\"\"Close a previously opened HDF file.\"\"\" self . h5file . close () self . _hdf_open_for_reading = False logging . info ( f \" { self . hdf_filename } closed.\" ) get_run ( self , run_id = None , run_name = None , filename = None , exploration_name = None ) Get a single result from a previous exploration. This function will load a single result from the HDF file. Use this function if you want to avoid loading all results to memory, which you can do using .load_results(all=True) . Note: This function will open the HDF for reading but will not close it afterwards! This is to speed up many sequential loads but it also means that you have to close the HDF file yourself. You can do this by using .close_hdf() . Parameters: Name Type Description Default run_id int, optional Unique id of the run. Has to be given if run_name is not given, defaults to None None run_name str, optional The name of the run. Has to be given if run_id is not given, defaults to None None filename str, optional Filename of the HDF with previous exploration results. Previously used filename will be used if not given, defaults to None None exploration_name str, optional Name of the exploration to load data from. Previously used exploration_name will be used if not given, defaults to None None Returns: Type Description dict Results of the run Source code in mopet/mopet.py def get_run ( self , run_id = None , run_name = None , filename = None , exploration_name = None ): \"\"\"Get a single result from a previous exploration. This function will load a single result from the HDF file. Use this function if you want to avoid loading all results to memory, which you can do using `.load_results(all=True)`. Note: This function will open the HDF for reading but will not close it afterwards! This is to speed up many sequential loads but it also means that you have to close the HDF file yourself. You can do this by using `.close_hdf()`. :param run_id: Unique id of the run. Has to be given if run_name is not given, defaults to None :type run_id: int, optional :param run_name: The name of the run. Has to be given if run_id is not given, defaults to None :type run_name: str, optional :param filename: Filename of the HDF with previous exploration results. Previously used filename will be used if not given, defaults to None :type filename: str, optional :param exploration_name: Name of the exploration to load data from. Previously used exploration_name will be used if not given, defaults to None :type exploration_name: str, optional :return: Results of the run :rtype: dict :raises: NoSuchExplorationError if hdf5 file does not contain `exploration_name` group. \"\"\" # get result by id or if not then by run_name (hdf_run) assert run_id is not None or run_name is not None , \"Either use `run_id` or `run_name`.\" if exploration_name : self . exploration_name = exploration_name if run_id is not None : run_name = self . RUN_PREFIX + str ( run_id ) if not self . _hdf_open_for_reading : self . _open_hdf ( filename ) try : run_results_group = self . h5file . get_node ( \"/\" + self . exploration_name , \"runs\" )[ run_name ] except NoSuchNodeError : raise ExplorationNotFoundError ( \"Exploration %s could not be found in HDF file %s \" . format ( self . exploration_name , self . hdf_filename ) ) result = self . _read_group_as_dict ( run_results_group ) return result load_results ( self , filename = None , exploration_name = None , aggregate = True , all = False ) Load results from previous explorations. This function will open an HDF file and look for an exploration. It will create a Pandas Dataframe object (accessible through the attribute .df ) with a list of all runs and their parameters. You can load the exploration results using following parameters: If aggregate==True , all scalar results (such as float or int ) from the exploration will be added to the Dataframe. If all==True , then all results, including arrays and other types, will be saved in the attribute .results . This can take up a lot of RAM since all results will be available. Only use this option if you know that you have enough memory. Otherwise, you might want to skip this and load results separately using the method .get_run() . Parameters: Name Type Description Default filename str, optional Filename of HDF file, uses default filename or previously used filename if not given, defaults to None None exploration_name str, optional Name of the exploration, same as the group names of the explorations in the HDF file, defaults to None None aggregate bool, optional Aggregate scalar results into the results Dataframe. If this option is enabled, defaults to True True all bool, optional Load all results into a dictionary available as the attribute .results . Can use a lot of RAM, defaults to False False Exceptions: Type Description Hdf5FileNotExistsError if file with filename does not exist. Source code in mopet/mopet.py def load_results ( self , filename = None , exploration_name = None , aggregate = True , all = False ): \"\"\"Load results from previous explorations. This function will open an HDF file and look for an exploration. It will create a Pandas `Dataframe` object (accessible through the attribute `.df`) with a list of all runs and their parameters. You can load the exploration results using following parameters: - If `aggregate==True`, all scalar results (such as `float` or `int`) from the exploration will be added to the Dataframe. - If `all==True`, then all results, including arrays and other types, will be saved in the attribute `.results`. This can take up a lot of RAM since all results will be available. Only use this option if you know that you have enough memory. Otherwise, you might want to skip this and load results separately using the method `.get_run()`. :param filename: Filename of HDF file, uses default filename or previously used filename if not given, defaults to None :type filename: str, optional :param exploration_name: Name of the exploration, same as the group names of the explorations in the HDF file, defaults to None :type exploration_name: str, optional :param aggregate: Aggregate scalar results into the results Dataframe. If this option is enabled, defaults to True :type aggregate: bool, optional :param all: Load all results into a dictionary available as the attribute `.results`. Can use a lot of RAM, defaults to False :type all: bool, optional :raises Hdf5FileNotExistsError: if file with `filename` does not exist. \"\"\" if exploration_name is None : exploration_name = self . exploration_name else : self . exploration_name = exploration_name self . _open_hdf ( filename = filename ) self . _load_all_results ( exploration_name , all = all ) self . _create_df () if aggregate : self . _aggregate_results ( exploration_name ) self . close_hdf () run ( self ) Start parameter exploration. TODO: Pass kwargs in run() to the exploration function Exceptions: Type Description ExplorationExistsError if exploration with same name already exists in HDF5 file. Source code in mopet/mopet.py def run ( self ): \"\"\"Start parameter exploration. TODO: Pass kwargs in run() to the exploration function :raises ExplorationExistsError: if exploration with same name already exists in HDF5 file. \"\"\" # Initialize ray self . _init_ray ( num_cpus = self . num_cpus , num_gpus = self . num_gpus ) # Create a list of all combinations of parameters from explore_params self . explore_params_list = self . _cartesian_product_dict ( self . explore_params ) # Initialize hdf storage self . _pre_storage_routine () # ----------------------------- # Set up all simulations # ----------------------------- # remember the time start_time = time . time () # a unique id for each run run_id = 0 # contains ray objects of each run ray_returns = {} # contains all exploration parameters of each run self . run_params_dict = {} logging . info ( f \"Starting { len ( self . explore_params_list ) } jobs.\" ) # cycle through all parameter combinations for update_params in tqdm . tqdm ( self . explore_params_list ): if self . full_params and self . default_params is not None : # load the default parameters run_params = copy . deepcopy ( self . default_params ) # and update them with the explored parameters run_params . update ( update_params ) else : run_params = copy . deepcopy ( update_params ) # start all ray jobs and remember the ray object # pylint: disable=no-member ray_returns [ run_id ] = _ray_remote . remote ( self . function , run_params ) # store this runs explore parameters self . run_params_dict [ run_id ] = copy . deepcopy ( update_params ) # increment the run id run_id += 1 # stop measuring time end_time = time . time () - start_time logging . info ( f \"Runs took { end_time } s to submit.\" ) # ----------------------------- # Reduce and store all results # ----------------------------- # remember the time start_time = time . time () # cycle through all returned ray objects for run_id , ray_return in tqdm . tqdm ( ray_returns . items ()): # get the appropriate parameters for this run run_param = self . run_params_dict [ run_id ] # queue object for storage self . _store_result ( run_id , ray_return , run_param ) # remove LOCAL_REFERENCE in form of ObjectId from ray's object store. ray_returns [ run_id ] = None # stop measuring time end_time = time . time () - start_time logging . info ( f \"Runs and storage took { end_time } s to complete.\" ) # tear down hdf storage self . _post_storage_routine () self . _shutdown_ray ()","title":"Mopet"},{"location":"mopet/#mopet","text":"","title":"Mopet"},{"location":"mopet/#mopet.mopet","text":"","title":"mopet.mopet"},{"location":"mopet/#mopet.mopet.Exploration","text":"","title":"Exploration"},{"location":"mopet/#mopet.mopet.Exploration.df","text":"Returns a dataframe with exploration results. Creates it from new if it doesn't exist yet. Returns: Type Description pandas.DataFrame Dataframe with exploration results","title":"df"},{"location":"mopet/#mopet.mopet.Exploration.__init__","text":"Defines a parameter exploration of a given function . Parameters: Name Type Description Default function function Function to evaluate at each run required explore_params dict Exploration parameters (individual) for each run required default_params dict Default (shared) parameters to load for each run, optional, defaults to None None exploration_name str, optional Name of the run, will create a name if left empty, defaults to None None hdf_filename str, optional Filename of the hdf storage file, defaults to None None num_cpus int Number of desired CPU cores passed to ray, defaults to None None num_gpus int Number of desired GPUs passed to ray, defaults to None None Returns: Type Description Exploration instance Source code in mopet/mopet.py def __init__ ( self , function , explore_params , default_params = None , exploration_name = None , hdf_filename = None , num_cpus : int = None , num_gpus : int = None , ): \"\"\"Defines a parameter exploration of a given `function`. :param function: Function to evaluate at each run :type function: function :param explore_params: Exploration parameters (individual) for each run :type explore_params: dict :param default_params: Default (shared) parameters to load for each run, optional, defaults to None :type default_params: dict :param exploration_name: Name of the run, will create a name if left empty, defaults to None :type exploration_name: str, optional :param hdf_filename: Filename of the hdf storage file, defaults to None :type hdf_filename: str, optional :param num_cpus: Number of desired CPU cores passed to ray, defaults to None :type num_cpus: int, optional :param num_gpus: Number of desired GPUs passed to ray, defaults to None :type num_gpus: int, optional :return: Exploration instance \"\"\" self . function = function self . results = {} self . results_params = [] if default_params is not None : self . default_params = copy . deepcopy ( default_params ) self . full_params = True else : self . default_params = None self . full_params = False self . explore_params = copy . deepcopy ( explore_params ) if exploration_name is None : exploration_name = \"exploration\" + datetime . datetime . now () . strftime ( \"_%Y_%m_ %d _%HH_%MM_%SS\" ) self . exploration_name = exploration_name if hdf_filename is None : hdf_filename = \"exploration.h5\" self . hdf_filename = hdf_filename self . dfResults = None # status self . _hdf_open_for_reading = False # List of all parameter combinations generated when exploration starts self . explore_params_list = None # Dict with runId as keys and explored parameter dict as value. # Will be filled when exploration starts. self . run_params_dict = {} # Dict with runId as keys and explored parameter dict as value. # Will be filled when calling `load_results`. self . params = {} # Ray configuration self . num_gpus = num_gpus self . num_cpus = num_cpus","title":"__init__()"},{"location":"mopet/#mopet.mopet.Exploration.close_hdf","text":"Close a previously opened HDF file. Source code in mopet/mopet.py def close_hdf ( self ): \"\"\"Close a previously opened HDF file.\"\"\" self . h5file . close () self . _hdf_open_for_reading = False logging . info ( f \" { self . hdf_filename } closed.\" )","title":"close_hdf()"},{"location":"mopet/#mopet.mopet.Exploration.get_run","text":"Get a single result from a previous exploration. This function will load a single result from the HDF file. Use this function if you want to avoid loading all results to memory, which you can do using .load_results(all=True) . Note: This function will open the HDF for reading but will not close it afterwards! This is to speed up many sequential loads but it also means that you have to close the HDF file yourself. You can do this by using .close_hdf() . Parameters: Name Type Description Default run_id int, optional Unique id of the run. Has to be given if run_name is not given, defaults to None None run_name str, optional The name of the run. Has to be given if run_id is not given, defaults to None None filename str, optional Filename of the HDF with previous exploration results. Previously used filename will be used if not given, defaults to None None exploration_name str, optional Name of the exploration to load data from. Previously used exploration_name will be used if not given, defaults to None None Returns: Type Description dict Results of the run Source code in mopet/mopet.py def get_run ( self , run_id = None , run_name = None , filename = None , exploration_name = None ): \"\"\"Get a single result from a previous exploration. This function will load a single result from the HDF file. Use this function if you want to avoid loading all results to memory, which you can do using `.load_results(all=True)`. Note: This function will open the HDF for reading but will not close it afterwards! This is to speed up many sequential loads but it also means that you have to close the HDF file yourself. You can do this by using `.close_hdf()`. :param run_id: Unique id of the run. Has to be given if run_name is not given, defaults to None :type run_id: int, optional :param run_name: The name of the run. Has to be given if run_id is not given, defaults to None :type run_name: str, optional :param filename: Filename of the HDF with previous exploration results. Previously used filename will be used if not given, defaults to None :type filename: str, optional :param exploration_name: Name of the exploration to load data from. Previously used exploration_name will be used if not given, defaults to None :type exploration_name: str, optional :return: Results of the run :rtype: dict :raises: NoSuchExplorationError if hdf5 file does not contain `exploration_name` group. \"\"\" # get result by id or if not then by run_name (hdf_run) assert run_id is not None or run_name is not None , \"Either use `run_id` or `run_name`.\" if exploration_name : self . exploration_name = exploration_name if run_id is not None : run_name = self . RUN_PREFIX + str ( run_id ) if not self . _hdf_open_for_reading : self . _open_hdf ( filename ) try : run_results_group = self . h5file . get_node ( \"/\" + self . exploration_name , \"runs\" )[ run_name ] except NoSuchNodeError : raise ExplorationNotFoundError ( \"Exploration %s could not be found in HDF file %s \" . format ( self . exploration_name , self . hdf_filename ) ) result = self . _read_group_as_dict ( run_results_group ) return result","title":"get_run()"},{"location":"mopet/#mopet.mopet.Exploration.load_results","text":"Load results from previous explorations. This function will open an HDF file and look for an exploration. It will create a Pandas Dataframe object (accessible through the attribute .df ) with a list of all runs and their parameters. You can load the exploration results using following parameters: If aggregate==True , all scalar results (such as float or int ) from the exploration will be added to the Dataframe. If all==True , then all results, including arrays and other types, will be saved in the attribute .results . This can take up a lot of RAM since all results will be available. Only use this option if you know that you have enough memory. Otherwise, you might want to skip this and load results separately using the method .get_run() . Parameters: Name Type Description Default filename str, optional Filename of HDF file, uses default filename or previously used filename if not given, defaults to None None exploration_name str, optional Name of the exploration, same as the group names of the explorations in the HDF file, defaults to None None aggregate bool, optional Aggregate scalar results into the results Dataframe. If this option is enabled, defaults to True True all bool, optional Load all results into a dictionary available as the attribute .results . Can use a lot of RAM, defaults to False False Exceptions: Type Description Hdf5FileNotExistsError if file with filename does not exist. Source code in mopet/mopet.py def load_results ( self , filename = None , exploration_name = None , aggregate = True , all = False ): \"\"\"Load results from previous explorations. This function will open an HDF file and look for an exploration. It will create a Pandas `Dataframe` object (accessible through the attribute `.df`) with a list of all runs and their parameters. You can load the exploration results using following parameters: - If `aggregate==True`, all scalar results (such as `float` or `int`) from the exploration will be added to the Dataframe. - If `all==True`, then all results, including arrays and other types, will be saved in the attribute `.results`. This can take up a lot of RAM since all results will be available. Only use this option if you know that you have enough memory. Otherwise, you might want to skip this and load results separately using the method `.get_run()`. :param filename: Filename of HDF file, uses default filename or previously used filename if not given, defaults to None :type filename: str, optional :param exploration_name: Name of the exploration, same as the group names of the explorations in the HDF file, defaults to None :type exploration_name: str, optional :param aggregate: Aggregate scalar results into the results Dataframe. If this option is enabled, defaults to True :type aggregate: bool, optional :param all: Load all results into a dictionary available as the attribute `.results`. Can use a lot of RAM, defaults to False :type all: bool, optional :raises Hdf5FileNotExistsError: if file with `filename` does not exist. \"\"\" if exploration_name is None : exploration_name = self . exploration_name else : self . exploration_name = exploration_name self . _open_hdf ( filename = filename ) self . _load_all_results ( exploration_name , all = all ) self . _create_df () if aggregate : self . _aggregate_results ( exploration_name ) self . close_hdf ()","title":"load_results()"},{"location":"mopet/#mopet.mopet.Exploration.run","text":"Start parameter exploration. TODO: Pass kwargs in run() to the exploration function Exceptions: Type Description ExplorationExistsError if exploration with same name already exists in HDF5 file. Source code in mopet/mopet.py def run ( self ): \"\"\"Start parameter exploration. TODO: Pass kwargs in run() to the exploration function :raises ExplorationExistsError: if exploration with same name already exists in HDF5 file. \"\"\" # Initialize ray self . _init_ray ( num_cpus = self . num_cpus , num_gpus = self . num_gpus ) # Create a list of all combinations of parameters from explore_params self . explore_params_list = self . _cartesian_product_dict ( self . explore_params ) # Initialize hdf storage self . _pre_storage_routine () # ----------------------------- # Set up all simulations # ----------------------------- # remember the time start_time = time . time () # a unique id for each run run_id = 0 # contains ray objects of each run ray_returns = {} # contains all exploration parameters of each run self . run_params_dict = {} logging . info ( f \"Starting { len ( self . explore_params_list ) } jobs.\" ) # cycle through all parameter combinations for update_params in tqdm . tqdm ( self . explore_params_list ): if self . full_params and self . default_params is not None : # load the default parameters run_params = copy . deepcopy ( self . default_params ) # and update them with the explored parameters run_params . update ( update_params ) else : run_params = copy . deepcopy ( update_params ) # start all ray jobs and remember the ray object # pylint: disable=no-member ray_returns [ run_id ] = _ray_remote . remote ( self . function , run_params ) # store this runs explore parameters self . run_params_dict [ run_id ] = copy . deepcopy ( update_params ) # increment the run id run_id += 1 # stop measuring time end_time = time . time () - start_time logging . info ( f \"Runs took { end_time } s to submit.\" ) # ----------------------------- # Reduce and store all results # ----------------------------- # remember the time start_time = time . time () # cycle through all returned ray objects for run_id , ray_return in tqdm . tqdm ( ray_returns . items ()): # get the appropriate parameters for this run run_param = self . run_params_dict [ run_id ] # queue object for storage self . _store_result ( run_id , ray_return , run_param ) # remove LOCAL_REFERENCE in form of ObjectId from ray's object store. ray_returns [ run_id ] = None # stop measuring time end_time = time . time () - start_time logging . info ( f \"Runs and storage took { end_time } s to complete.\" ) # tear down hdf storage self . _post_storage_routine () self . _shutdown_ray ()","title":"run()"},{"location":"examples/minimal_example/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # change into the root directory of the project import os if os . getcwd () . split ( \"/\" )[ - 1 ] == \"examples\" : os . chdir ( '..' ) % load_ext autoreload % autoreload 2 import logging logger = logging . getLogger () logger . setLevel ( logging . INFO ) import numpy as np ! pip install matplotlib import matplotlib.pyplot as plt import mopet # a nice color map plt . rcParams [ 'image.cmap' ] = 'plasma' ! rm exploration . h5 def evalFunction ( params ): result_float = abs (( params [ \"x\" ] ** 2 + params [ \"y\" ] ** 2 ) - 1 ) result_array = np . random . randn ( np . random . randint ( 1 , 131 ), np . random . randint ( 1 , 5000 )) result = {} result [ \"float_result\" ] = result_float result [ \"array_result\" ] = result_array return result explore_params = { \"x\" : np . linspace ( - 2 , 2 , 21 ), \"y\" : np . linspace ( - 2 , 2 , 21 )} ex = mopet . Exploration ( evalFunction , explore_params ) Requirement already satisfied: matplotlib in /Users/caglar/anaconda/lib/python3.7/site-packages (3.1.3) Requirement already satisfied: python-dateutil>=2.1 in /Users/caglar/anaconda/lib/python3.7/site-packages (from matplotlib) (2.8.0) Requirement already satisfied: cycler>=0.10 in /Users/caglar/anaconda/lib/python3.7/site-packages (from matplotlib) (0.10.0) Requirement already satisfied: numpy>=1.11 in /Users/caglar/anaconda/lib/python3.7/site-packages (from matplotlib) (1.18.1) Requirement already satisfied: kiwisolver>=1.0.1 in /Users/caglar/anaconda/lib/python3.7/site-packages (from matplotlib) (1.1.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/caglar/anaconda/lib/python3.7/site-packages (from matplotlib) (2.4.0) Requirement already satisfied: six>=1.5 in /Users/caglar/anaconda/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.13.0) Requirement already satisfied: setuptools in /Users/caglar/anaconda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0) ex . run () 2020-04-20 21:46:37,480 INFO resource_spec.py:212 -- Starting Ray with 3.27 GiB memory available for workers and up to 1.64 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>). 2020-04-20 21:46:37,952 INFO services.py:1093 -- View the Ray dashboard at localhost:8265 INFO:root:Starting 441 jobs. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 441/441 [00:03<00:00, 118.57it/s] INFO:root:Runs took 3.727734088897705 s to submit. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 441/441 [00:01<00:00, 224.70it/s] INFO:root:Runs and storage took 1.9686880111694336 s to complete. ex . load_results ( all = True ) INFO:root:exploration.h5 opened for reading. INFO:root:Gettings runs of exploration ``exploration_2020_04_20_21H_46M_37S`` 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 441/441 [00:01<00:00, 379.44it/s] INFO:root:Creating new results DataFrame INFO:root:Aggregating scalar results ... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 441/441 [00:00<00:00, 502.89it/s] INFO:root:exploration.h5 closed. len ( ex . results ) 441 ex . get_run ( 0 ) INFO:root:exploration.h5 opened for reading. {'array_result': array([[ 5.97391695e-01, -1.85095801e+00, 6.36841315e-01, ..., -3.62676558e-01, -2.09826990e-01, 1.40291897e+00], [ 1.75607948e+00, 1.05066847e+00, 8.95172810e-01, ..., -7.77506344e-01, -9.06090056e-01, 9.48383121e-04], [-2.20332623e-01, 6.78649005e-01, -8.93520258e-01, ..., -2.22872632e+00, -1.19226748e+00, -1.38276576e-01], ..., [ 7.76605125e-01, -5.22077056e-01, 4.18754799e-01, ..., 1.15120920e+00, -3.58705657e-01, -9.63737910e-01], [ 2.55826835e+00, -5.37584683e-01, 6.39454329e-01, ..., -1.99267714e-01, 5.92317635e-01, -7.96497612e-01], [ 3.18325277e-01, 2.13536242e+00, 1.12065066e+00, ..., -4.70540293e-01, 1.08954728e+00, 4.97407056e-01]]), 'float_result': 7.0} for r in ex . df . index : ex . df . loc [ r , \"mean_array_result\" ] = np . mean ( ex . get_run ( r )[ 'array_result' ]) ex . df . dropna ( axis = 'columns' , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y float_result mean_array_result 0 -2 -2 7.00 0.006656 1 -2 -1.8 6.24 0.000567 10 -2 0 3.00 0.002344 100 -1.2 1.2 1.88 -0.000210 101 -1.2 1.4 2.40 0.003098 ... ... ... ... ... 95 -1.2 0.2 0.48 0.003001 96 -1.2 0.4 0.60 -0.000337 97 -1.2 0.6 0.80 0.001959 98 -1.2 0.8 1.08 -0.010278 99 -1.2 1 1.44 0.002511 441 rows \u00d7 4 columns pivoted = ex . df . pivot_table ( values = 'float_result' , index = 'y' , columns = 'x' , aggfunc = 'first' ) plt . imshow ( pivoted , \\ extent = [ min ( ex . df . x ), max ( ex . df . x ), min ( ex . df . y ), max ( ex . df . y )], origin = 'lower' ) plt . colorbar ( label = 'Distance from unit circle' ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) Text(0, 0.5, 'y')","title":"Minimal example"},{"location":"examples/neurolib_brain_network/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # change into the root directory of the project import os if os . getcwd () . split ( \"/\" )[ - 1 ] == \"examples\" : os . chdir ( '..' ) % load_ext autoreload % autoreload 2 import logging logger = logging . getLogger () logger . setLevel ( logging . INFO ) #!pip install matplotlib import matplotlib.pyplot as plt import numpy as np # a nice color map plt . rcParams [ 'image.cmap' ] = 'plasma' #!pip install neurolib from neurolib.models.hopf import HopfModel from neurolib.models.aln import ALNModel from neurolib.utils.loadData import Dataset import neurolib.utils.functions as func ds = Dataset ( \"hcp\" ) import mopet INFO:root:Loading dataset hcp from /Users/caglar/anaconda/lib/python3.7/site-packages/neurolib/utils/../data/datasets/hcp. INFO:root:Dataset hcp loaded. model = HopfModel ( Cmat = ds . Cmat , Dmat = ds . Dmat ) #model = HopfModel(Cmat = ds.Cmat, Dmat = ds.Dmat) model . params [ 'duration' ] = 0.2 * 60 * 1000 #ms model . params [ 'w' ] = 1.0 model . params [ 'K_gl' ] = 1.0 model . params [ 'a' ] = - 1.0 model . params [ 'sigma_ou' ] = 0.01 model . params [ 'dt' ] = 0.05 INFO:root:hopf: Model initialized. model = ALNModel ( Cmat = ds . Cmat , Dmat = ds . Dmat ) model . params [ 'duration' ] = 0.2 * 60 * 1000 model . params [ 'mue_ext_mean' ] = 1.57 model . params [ 'mui_ext_mean' ] = 1.6 # We set an appropriate level of noise model . params [ 'sigma_ou' ] = 0.09 # And turn on adaptation with a low value of spike-triggered adaptation currents. model . params [ 'b' ] = 5.0 INFO:root:aln: Model initialized. model . run ( bold = True , chunkwise = True ) INFO:root:aln: BOLD model initialized. plt . plot ( model . output . T ); plt . imshow ( func . fc ( model . BOLD . BOLD [:, model . BOLD . t > 5000 ])) <matplotlib.image.AxesImage at 0x12cebe518> def evaluateSimulation ( params ): model . params . update ( params ) defaultDuration = model . params [ 'duration' ] invalid_result = { \"fc\" : [ 0 ] * len ( ds . BOLDs )} logging . info ( \"Running stage 1\" ) # -------- stage wise simulation -------- # Stage 1 : simulate for a few seconds to see if there is any activity # --------------------------------------- model . params [ 'duration' ] = 3 * 1000. model . run () # check if stage 1 was successful amplitude = np . max ( model . output [:, model . t > 500 ]) - np . min ( model . output [:, model . t > 500 ]) if amplitude < 0.05 : invalid_result = { \"fc\" : 0 } return invalid_result logging . info ( \"Running stage 2\" ) # Stage 2: simulate BOLD for a few seconds to see if it moves # --------------------------------------- model . params [ 'duration' ] = 20 * 1000. model . run ( bold = True , chunkwise = True ) if np . std ( model . BOLD . BOLD [:, 5 : 10 ]) < 0.0001 : invalid_result = { \"fc\" : - 1 } return invalid_result logging . info ( \"Running stage 3\" ) # Stage 3: full and final simulation # --------------------------------------- model . params [ 'duration' ] = defaultDuration model . run ( bold = True , chunkwise = True ) # -------- evaluation here -------- scores = [] for i , fc in enumerate ( ds . FCs ): #range(len(ds.FCs)): fc_score = func . matrix_correlation ( func . fc ( model . BOLD . BOLD [:, 5 :]), fc ) scores . append ( fc_score ) meanScore = np . mean ( scores ) result_dict = { \"fc\" : meanScore } return result_dict np . isnan ( np . sum ( model . BOLD . BOLD )) False model . params [ 'duration' ] = 20 * 1000. evaluateSimulation ( model . params ) INFO:root:Running stage 1 INFO:root:aln: BOLD model initialized. INFO:root:Running stage 2 INFO:root:aln: BOLD model initialized. INFO:root:Running stage 3 INFO:root:aln: BOLD model initialized. {'fc': 0.34972347902919626} params = model . params explore_params = { \"a\" : np . linspace ( - 2 , 1.5 , 6 ) , \"K_gl\" : np . linspace ( 0 , 2 , 4 ) , \"sigma_ou\" : np . linspace ( 0.1 , 0.5 , 2 ) } ex = mopet . Exploration ( evaluateSimulation , explore_params , default_params = params ) ex . run () 2020-03-14 00:00:22,500 INFO resource_spec.py:212 -- Starting Ray with 3.37 GiB memory available for workers and up to 1.7 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>). 2020-03-14 00:00:22,910 INFO services.py:1093 -- View the Ray dashboard at localhost:8266 INFO:root:Starting 48 jobs. 0%| | 0/48 [00:00<?, ?it/s] 2%|\u258f | 1/48 [00:00<00:15, 3.11it/s] 4%|\u258d | 2/48 [00:00<00:14, 3.26it/s] 6%|\u258b | 3/48 [00:00<00:13, 3.43it/s] 8%|\u258a | 4/48 [00:01<00:13, 3.17it/s] 10%|\u2588 | 5/48 [00:01<00:16, 2.60it/s] 12%|\u2588\u258e | 6/48 [00:01<00:14, 2.96it/s] 15%|\u2588\u258d | 7/48 [00:02<00:12, 3.21it/s] 17%|\u2588\u258b | 8/48 [00:02<00:12, 3.17it/s] 19%|\u2588\u2589 | 9/48 [00:02<00:13, 2.96it/s] 21%|\u2588\u2588 | 10/48 [00:03<00:14, 2.67it/s] 23%|\u2588\u2588\u258e | 11/48 [00:03<00:12, 3.04it/s] 25%|\u2588\u2588\u258c | 12/48 [00:03<00:10, 3.38it/s] 27%|\u2588\u2588\u258b | 13/48 [00:04<00:10, 3.36it/s] 29%|\u2588\u2588\u2589 | 14/48 [00:04<00:09, 3.73it/s] 31%|\u2588\u2588\u2588\u258f | 15/48 [00:04<00:08, 3.87it/s] 33%|\u2588\u2588\u2588\u258e | 16/48 [00:04<00:07, 4.12it/s] 35%|\u2588\u2588\u2588\u258c | 17/48 [00:04<00:06, 4.53it/s] 38%|\u2588\u2588\u2588\u258a | 18/48 [00:05<00:06, 4.39it/s] 40%|\u2588\u2588\u2588\u2589 | 19/48 [00:05<00:06, 4.36it/s] 42%|\u2588\u2588\u2588\u2588\u258f | 20/48 [00:05<00:07, 3.95it/s] 44%|\u2588\u2588\u2588\u2588\u258d | 21/48 [00:05<00:06, 4.08it/s] 46%|\u2588\u2588\u2588\u2588\u258c | 22/48 [00:06<00:07, 3.38it/s] 48%|\u2588\u2588\u2588\u2588\u258a | 23/48 [00:06<00:07, 3.47it/s] 50%|\u2588\u2588\u2588\u2588\u2588 | 24/48 [00:06<00:07, 3.37it/s] 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 25/48 [00:07<00:06, 3.82it/s] 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 26/48 [00:07<00:06, 3.51it/s] 56%|\u2588\u2588\u2588\u2588\u2588\u258b | 27/48 [00:07<00:05, 3.72it/s] 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 28/48 [00:08<00:05, 3.68it/s] 60%|\u2588\u2588\u2588\u2588\u2588\u2588 | 29/48 [00:08<00:04, 3.89it/s] 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 30/48 [00:08<00:04, 4.05it/s] 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 31/48 [00:08<00:04, 4.03it/s] 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 32/48 [00:08<00:03, 4.14it/s] 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 33/48 [00:09<00:03, 4.35it/s] 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 34/48 [00:09<00:03, 3.74it/s] 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 35/48 [00:09<00:03, 3.98it/s] 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 36/48 [00:09<00:02, 4.04it/s] 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 37/48 [00:10<00:02, 3.97it/s] 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 38/48 [00:10<00:02, 3.42it/s] 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 39/48 [00:10<00:02, 3.74it/s] 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 40/48 [00:11<00:01, 4.02it/s] 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 41/48 [00:11<00:01, 4.20it/s] 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 42/48 [00:11<00:01, 4.51it/s] 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 43/48 [00:11<00:01, 4.07it/s] 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 44/48 [00:12<00:01, 3.83it/s] 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 45/48 [00:12<00:00, 3.90it/s] 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 46/48 [00:12<00:00, 4.02it/s] 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 47/48 [00:12<00:00, 3.70it/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [00:13<00:00, 3.68it/s] INFO:root:Runs took 13.049254894256592 s to submit. WARNING:root:Could not store dict entry model (type: <class 'str'>) WARNING:root:Could not store dict entry name (type: <class 'str'>) WARNING:root:Could not store dict entry description (type: <class 'str'>) 0%| | 0/48 [00:00<?, ?it/s] 2%|\u258f | 1/48 [00:33<26:09, 33.40s/it] 4%|\u258d | 2/48 [00:33<17:59, 23.47s/it] 12%|\u2588\u258e | 6/48 [00:33<11:30, 16.44s/it] 17%|\u2588\u258b | 8/48 [00:34<07:41, 11.54s/it] 19%|\u2588\u2589 | 9/48 [01:07<11:46, 18.11s/it] 21%|\u2588\u2588 | 10/48 [01:08<08:12, 12.95s/it] 23%|\u2588\u2588\u258e | 11/48 [01:08<05:37, 9.13s/it] 27%|\u2588\u2588\u258b | 13/48 [01:14<04:16, 7.32s/it] 35%|\u2588\u2588\u2588\u258c | 17/48 [01:44<03:47, 7.34s/it] 38%|\u2588\u2588\u2588\u258a | 18/48 [01:45<02:45, 5.52s/it] 40%|\u2588\u2588\u2588\u2589 | 19/48 [01:45<01:55, 3.97s/it] 42%|\u2588\u2588\u2588\u2588\u258f | 20/48 [01:46<01:19, 2.83s/it] 46%|\u2588\u2588\u2588\u2588\u258c | 22/48 [01:46<00:52, 2.00s/it] 48%|\u2588\u2588\u2588\u2588\u258a | 23/48 [01:52<01:20, 3.23s/it] 50%|\u2588\u2588\u2588\u2588\u2588 | 24/48 [01:52<00:58, 2.45s/it] 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 25/48 [02:21<03:58, 10.35s/it] 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 26/48 [02:23<02:51, 7.79s/it] 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 28/48 [02:24<01:51, 5.55s/it] 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 30/48 [02:24<01:10, 3.92s/it] 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 31/48 [02:29<01:11, 4.19s/it] 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 32/48 [02:30<00:53, 3.32s/it] 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 33/48 [02:58<02:42, 10.84s/it] 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 34/48 [03:01<01:58, 8.45s/it] 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 38/48 [03:01<00:59, 5.93s/it] 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 39/48 [03:06<00:49, 5.54s/it] 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 40/48 [03:08<00:34, 4.34s/it] 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 41/48 [03:34<01:15, 10.85s/it] 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 42/48 [03:36<00:48, 8.15s/it] 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 43/48 [03:36<00:28, 5.76s/it] 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 44/48 [03:36<00:16, 4.24s/it] 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 46/48 [03:37<00:05, 3.00s/it] 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 47/48 [03:39<00:02, 2.69s/it] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48/48 [03:40<00:00, 4.59s/it] INFO:root:Runs and storage took 220.27872014045715 s to complete. ex . load_results () INFO:root:exploration.h5 opened for reading. INFO:root:Gettings runs of exploration ``exploration_2020_03_13_23H_58M_12S`` INFO:root:16 results loaded. INFO:root:Creating new results DataFrame INFO:root:exploration.h5 closed. ex . results {0: {'fc': 0.14816326145112374}, 1: {'fc': 0.002602004174608284}, 10: {'fc': 0.23827704520224735}, 11: {'fc': 0.025222663622377677}, 12: {'fc': 0.03178939679645592}, 13: {'fc': 0.06044585543052911}, 14: {'fc': 0.0032558664899791467}, 15: {'fc': 0.010869472950270872}, 2: {'fc': 0.17255279155560974}, 3: {'fc': 0.06885798944091048}, 4: {'fc': 0.09379917381439488}, 5: {'fc': -0.0032952264898001835}, 6: {'fc': 0.07305279567940047}, 7: {'fc': -0.010722083537767563}, 8: {'fc': 0.26763011003174636}, 9: {'fc': 0.06793740182831143}} ex . params {0: {'K_gl': 0.0, 'a': -2.0, 'sigma_ou': 0.1}, 1: {'K_gl': 0.0, 'a': -2.0, 'sigma_ou': 0.5}, 10: {'K_gl': 2.0, 'a': 0.3333333333333335, 'sigma_ou': 0.1}, 11: {'K_gl': 2.0, 'a': 0.3333333333333335, 'sigma_ou': 0.5}, 12: {'K_gl': 0.0, 'a': 1.5, 'sigma_ou': 0.1}, 13: {'K_gl': 0.0, 'a': 1.5, 'sigma_ou': 0.5}, 14: {'K_gl': 2.0, 'a': 1.5, 'sigma_ou': 0.1}, 15: {'K_gl': 2.0, 'a': 1.5, 'sigma_ou': 0.5}, 2: {'K_gl': 2.0, 'a': -2.0, 'sigma_ou': 0.1}, 3: {'K_gl': 2.0, 'a': -2.0, 'sigma_ou': 0.5}, 4: {'K_gl': 0.0, 'a': -0.8333333333333333, 'sigma_ou': 0.1}, 5: {'K_gl': 0.0, 'a': -0.8333333333333333, 'sigma_ou': 0.5}, 6: {'K_gl': 2.0, 'a': -0.8333333333333333, 'sigma_ou': 0.1}, 7: {'K_gl': 2.0, 'a': -0.8333333333333333, 'sigma_ou': 0.5}, 8: {'K_gl': 0.0, 'a': 0.3333333333333335, 'sigma_ou': 0.1}, 9: {'K_gl': 0.0, 'a': 0.3333333333333335, 'sigma_ou': 0.5}} ex . df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } K_gl a sigma_ou 0 0 -2 0.1 1 0 -2 0.5 10 2 0.333333 0.1 11 2 0.333333 0.5 12 0 1.5 0.1 13 0 1.5 0.5 14 2 1.5 0.1 15 2 1.5 0.5 2 2 -2 0.1 3 2 -2 0.5 4 0 -0.833333 0.1 5 0 -0.833333 0.5 6 2 -0.833333 0.1 7 2 -0.833333 0.5 8 0 0.333333 0.1 9 0 0.333333 0.5 for i in ex . df . index : ex . df . loc [ i , 'bold_cc' ] = np . mean ( ex . results [ i ][ 'fc' ]) ex . df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } K_gl a sigma_ou bold_cc 0 0 -2 0.1 0.148163 1 0 -2 0.5 0.002602 10 2 0.333333 0.1 0.238277 11 2 0.333333 0.5 0.025223 12 0 1.5 0.1 0.031789 13 0 1.5 0.5 0.060446 14 2 1.5 0.1 0.003256 15 2 1.5 0.5 0.010869 2 2 -2 0.1 0.172553 3 2 -2 0.5 0.068858 4 0 -0.833333 0.1 0.093799 5 0 -0.833333 0.5 -0.003295 6 2 -0.833333 0.1 0.073053 7 2 -0.833333 0.5 -0.010722 8 0 0.333333 0.1 0.267630 9 0 0.333333 0.5 0.067937 sigma_selectors = np . unique ( ex . df . sigma_ou ) for s in sigma_selectors : df = ex . df [( ex . df . sigma_ou == s )] pivotdf = df . pivot_table ( values = 'bold_cc' , index = 'K_gl' , columns = 'a' ) plt . imshow ( pivotdf , \\ extent = [ min ( df . a ), max ( df . a ), min ( df . K_gl ), max ( df . K_gl )], origin = 'lower' , aspect = 'auto' ) plt . colorbar ( label = 'Mean correlation to empirical rs-FC' ) plt . xlabel ( \"a\" ) plt . ylabel ( \"K_gl\" ) plt . title ( \"$\\sigma_ {ou} $\" + \"= {} \" . format ( s )) plt . show ()","title":"Neurolib brain network"},{"location":"examples/neurolib_example/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # change into the root directory of the project import os if os . getcwd () . split ( \"/\" )[ - 1 ] == \"examples\" : os . chdir ( '..' ) % load_ext autoreload % autoreload 2 import logging logger = logging . getLogger () logger . setLevel ( logging . INFO ) ! pip install matplotlib import matplotlib.pyplot as plt import numpy as np # a nice color map plt . rcParams [ 'image.cmap' ] = 'plasma' ! pip install neurolib from neurolib.models.aln import ALNModel from neurolib.utils.loadData import Dataset ds = Dataset ( \"hcp\" ) import mopet ! rm exploration . h5 #model = ALNModel(Cmat = ds.Cmat, Dmat = ds.Dmat) model = ALNModel () model . params . duration = 1 * 1000 def evalFunction ( params ): model . params . update ( params ) model . run () return model . outputs params = model . params explore_params = { \"mue_ext_mean\" : np . linspace ( 0 , 3 , 31 ), \"mui_ext_mean\" : np . linspace ( 0 , 3 , 31 )} ex = mopet . Exploration ( evalFunction , explore_params , default_params = params ) ex . run () 2020-03-13 03:30:57,591 INFO resource_spec.py:212 -- Starting Ray with 3.86 GiB memory available for workers and up to 1.93 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>). 2020-03-13 03:30:58,175 INFO services.py:1093 -- View the Ray dashboard at localhost:8266 INFO:root:Runs took 20.913066148757935 s to submit. WARNING:root:Could not store dict entry model (type: <class 'str'>) WARNING:root:Could not store dict entry name (type: <class 'str'>) WARNING:root:Could not store dict entry description (type: <class 'str'>) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 961/961 [00:03<00:00, 305.00it/s] INFO:root:Runs and storage took 3.152482032775879 s to complete. ex . load_results () ex . results ex . df [ \"result\" ] = None for r in ex . df . index : t = ex . results [ r ][ 't' ] rates_exc = ex . results [ r ][ 'rates_exc' ] ex . df . loc [ r , \"result\" ] = np . max ( rates_exc [:, t > 500 ]) ex . df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mue_ext_mean mui_ext_mean result 0 0 0 0.0355327 1 0 0.1 0.0230245 10 0 1 4.47908e-08 100 0.3 0.7 0.00575565 101 0.3 0.8 0.00123482 ... ... ... ... 96 0.3 0.3 0.766613 960 3 3 63.3969 97 0.3 0.4 0.225154 98 0.3 0.5 0.0702888 99 0.3 0.6 0.0217004 961 rows \u00d7 3 columns pivoted = ex . df . pivot_table ( values = 'result' , index = 'mui_ext_mean' , columns = 'mue_ext_mean' , aggfunc = 'first' ) plt . imshow ( pivoted , \\ extent = [ min ( ex . df . mue_ext_mean ), max ( ex . df . mue_ext_mean ), min ( ex . df . mui_ext_mean ), max ( ex . df . mui_ext_mean )], origin = 'lower' ) plt . colorbar ( label = 'Maximum firing rate' ) plt . xlabel ( \"Input to E\" ) plt . ylabel ( \"Input to I\" ) Text(0, 0.5, 'Input to I')","title":"Neurolib example"},{"location":"examples/readme_example/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # change into the root directory of the project import os if os . getcwd () . split ( \"/\" )[ - 1 ] == \"examples\" : os . chdir ( '..' ) % load_ext autoreload % autoreload 2 import logging logger = logging . getLogger () logger . setLevel ( logging . INFO ) # first we define an toy evaluation function def distance_from_circle ( params ): # let's simply calculate the distance of # the x-y parameters to the unit circle distance = abs (( params [ \"x\" ] ** 2 + params [ \"y\" ] ** 2 ) - 1 ) # we package the result in a dictionary result = { \"result\" : distance } return result import numpy as np import mopet explore_params = { \"x\" : np . linspace ( - 2 , 2 , 21 ), \"y\" : np . linspace ( - 2 , 2 , 21 )} ex = mopet . Exploration ( distance_from_circle , explore_params ) ex . run () 2020-03-13 03:39:19,152 INFO resource_spec.py:212 -- Starting Ray with 4.64 GiB memory available for workers and up to 2.34 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>). 2020-03-13 03:39:19,566 INFO services.py:1093 -- View the Ray dashboard at localhost:8268 INFO:root:Runs took 3.4930810928344727 s to submit. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 441/441 [00:01<00:00, 426.57it/s] INFO:root:Runs and storage took 1.0414140224456787 s to complete. ex . load_results () INFO:root:exploration.h5 opened for reading. INFO:root:Gettings runs of exploration ``exploration_2020_03_13_03H_39M_18S`` INFO:root:441 results loaded. INFO:root:Creating new results DataFrame INFO:root:exploration.h5 closed. ex . df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x y 0 -2 -2 1 -2 -1.8 10 -2 0 100 -1.2 1.2 101 -1.2 1.4 ... ... ... 95 -1.2 0.2 96 -1.2 0.4 97 -1.2 0.6 98 -1.2 0.8 99 -1.2 1 441 rows \u00d7 2 columns ex . df [ \"result\" ] = None for r in ex . df . index : ex . df . loc [ r , \"result\" ] = ex . results [ r ][ 'result' ] pivoted = ex . df . pivot_table ( values = 'result' , index = 'y' , columns = 'x' , aggfunc = 'first' ) import pandas as pd pd . set_option ( 'display.float_format' , lambda x : ' %.3f ' % x ) pivoted .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x -2.000 -1.800 -1.600 -1.400 -1.200 -1.000 -0.800 -0.600 -0.400 -0.200 ... 0.200 0.400 0.600 0.800 1.000 1.200 1.400 1.600 1.800 2.000 y -2.000 7.000 6.240 5.560 4.960 4.440 4.000 3.640 3.360 3.160 3.040 ... 3.040 3.160 3.360 3.640 4.000 4.440 4.960 5.560 6.240 7.000 -1.800 6.240 5.480 4.800 4.200 3.680 3.240 2.880 2.600 2.400 2.280 ... 2.280 2.400 2.600 2.880 3.240 3.680 4.200 4.800 5.480 6.240 -1.600 5.560 4.800 4.120 3.520 3.000 2.560 2.200 1.920 1.720 1.600 ... 1.600 1.720 1.920 2.200 2.560 3.000 3.520 4.120 4.800 5.560 -1.400 4.960 4.200 3.520 2.920 2.400 1.960 1.600 1.320 1.120 1.000 ... 1.000 1.120 1.320 1.600 1.960 2.400 2.920 3.520 4.200 4.960 -1.200 4.440 3.680 3.000 2.400 1.880 1.440 1.080 0.800 0.600 0.480 ... 0.480 0.600 0.800 1.080 1.440 1.880 2.400 3.000 3.680 4.440 -1.000 4.000 3.240 2.560 1.960 1.440 1.000 0.640 0.360 0.160 0.040 ... 0.040 0.160 0.360 0.640 1.000 1.440 1.960 2.560 3.240 4.000 -0.800 3.640 2.880 2.200 1.600 1.080 0.640 0.280 0.000 0.200 0.320 ... 0.320 0.200 0.000 0.280 0.640 1.080 1.600 2.200 2.880 3.640 -0.600 3.360 2.600 1.920 1.320 0.800 0.360 0.000 0.280 0.480 0.600 ... 0.600 0.480 0.280 0.000 0.360 0.800 1.320 1.920 2.600 3.360 -0.400 3.160 2.400 1.720 1.120 0.600 0.160 0.200 0.480 0.680 0.800 ... 0.800 0.680 0.480 0.200 0.160 0.600 1.120 1.720 2.400 3.160 -0.200 3.040 2.280 1.600 1.000 0.480 0.040 0.320 0.600 0.800 0.920 ... 0.920 0.800 0.600 0.320 0.040 0.480 1.000 1.600 2.280 3.040 0.000 3.000 2.240 1.560 0.960 0.440 0.000 0.360 0.640 0.840 0.960 ... 0.960 0.840 0.640 0.360 0.000 0.440 0.960 1.560 2.240 3.000 0.200 3.040 2.280 1.600 1.000 0.480 0.040 0.320 0.600 0.800 0.920 ... 0.920 0.800 0.600 0.320 0.040 0.480 1.000 1.600 2.280 3.040 0.400 3.160 2.400 1.720 1.120 0.600 0.160 0.200 0.480 0.680 0.800 ... 0.800 0.680 0.480 0.200 0.160 0.600 1.120 1.720 2.400 3.160 0.600 3.360 2.600 1.920 1.320 0.800 0.360 0.000 0.280 0.480 0.600 ... 0.600 0.480 0.280 0.000 0.360 0.800 1.320 1.920 2.600 3.360 0.800 3.640 2.880 2.200 1.600 1.080 0.640 0.280 0.000 0.200 0.320 ... 0.320 0.200 0.000 0.280 0.640 1.080 1.600 2.200 2.880 3.640 1.000 4.000 3.240 2.560 1.960 1.440 1.000 0.640 0.360 0.160 0.040 ... 0.040 0.160 0.360 0.640 1.000 1.440 1.960 2.560 3.240 4.000 1.200 4.440 3.680 3.000 2.400 1.880 1.440 1.080 0.800 0.600 0.480 ... 0.480 0.600 0.800 1.080 1.440 1.880 2.400 3.000 3.680 4.440 1.400 4.960 4.200 3.520 2.920 2.400 1.960 1.600 1.320 1.120 1.000 ... 1.000 1.120 1.320 1.600 1.960 2.400 2.920 3.520 4.200 4.960 1.600 5.560 4.800 4.120 3.520 3.000 2.560 2.200 1.920 1.720 1.600 ... 1.600 1.720 1.920 2.200 2.560 3.000 3.520 4.120 4.800 5.560 1.800 6.240 5.480 4.800 4.200 3.680 3.240 2.880 2.600 2.400 2.280 ... 2.280 2.400 2.600 2.880 3.240 3.680 4.200 4.800 5.480 6.240 2.000 7.000 6.240 5.560 4.960 4.440 4.000 3.640 3.360 3.160 3.040 ... 3.040 3.160 3.360 3.640 4.000 4.440 4.960 5.560 6.240 7.000 21 rows \u00d7 21 columns import matplotlib.pyplot as plt # a nice color map plt . figure ( dpi = 100 ) plt . rcParams [ 'image.cmap' ] = 'plasma' plt . imshow ( pivoted , \\ extent = [ min ( ex . df . x ), max ( ex . df . x ), min ( ex . df . y ), max ( ex . df . y )], origin = 'lower' ) plt . colorbar ( label = 'Distance from unit circle' ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y\" ) Text(0, 0.5, 'y')","title":"Readme example"}]}